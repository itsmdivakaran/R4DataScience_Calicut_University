# Probability

## Introduction

Probability provides the **mathematical framework for quantifying uncertainty**. While statistics focuses on describing and analyzing observed data, probability deals with the underlying randomness that generates the data. Together, probability and statistics form the foundation of modern data analysis, inference, and decision-making.

This chapter introduces the **core concepts of probability theory** required for statistical analysis. The emphasis is on clear definitions, intuitive understanding, and simple illustrations, with light use of R to reinforce concepts. Statistical inference based on probability will be covered in the next chapter.

---

## Random Experiments

A **random experiment** is a process that produces an outcome which cannot be predicted with certainty, even if the experiment is repeated under identical conditions.

Examples include:

- Tossing a coin
- Rolling a die
- Selecting a patient randomly from a population
- Measuring response time in an experiment

The defining feature of a random experiment is **uncertainty in outcomes**.

---

## Sample Space

The **sample space**, denoted by $S$, is the set of all possible outcomes of a random experiment.

Examples:

- Coin toss:  
  $S = \{H, T\}$

- Die roll:  
  $S = \{1, 2, 3, 4, 5, 6\}$

In R, a sample space can be represented using vectors:

```r
sample_space <- c("H", "T")
sample_space
```

---

## Events

An **event** is any subset of the sample space. Events may consist of:

- A single outcome (simple event)
- Multiple outcomes (compound event)

Example (die roll):

- Event A = rolling an even number  
  $A = \{2, 4, 6\}$

```r
A <- c(2, 4, 6)
```

---

## Types of Events

### Mutually Exclusive Events

Two events are **mutually exclusive** if they cannot occur at the same time.

Example:

- Rolling a 2 and rolling a 5 on a single die

---

### Exhaustive Events

A set of events is **exhaustive** if at least one of them must occur.

Example:

- Even number or odd number in a die roll

---

## Classical Definition of Probability

If a random experiment has a finite number of equally likely outcomes, the probability of an event $A$ is defined as:

$$
P(A) = \frac{\text{Number of favorable outcomes}}{\text{Total number of outcomes}}
$$

Example:

Probability of rolling an even number on a fair die:

$$
P(A) = \frac{3}{6} = 0.5
$$

---

## Axiomatic Definition of Probability

Modern probability theory is based on three axioms proposed by **Kolmogorov**:

1. For any event $A$, $P(A) \ge 0$
2. $P(S) = 1$
3. For mutually exclusive events $A_1, A_2, \dots$:  
   $P(A_1 \cup A_2 \cup \dots) = \sum P(A_i)$

These axioms provide a rigorous mathematical foundation for probability.

---

## Probability of Complementary Events

The **complement** of an event $A$, denoted by $A^c$, consists of all outcomes not in $A$.

$$
P(A^c) = 1 - P(A)
$$

Example:

If $P(A) = 0.7$, then:

$$
P(A^c) = 0.3
$$

---

## Addition Law of Probability

For any two events $A$ and $B$:

$$
P(A \cup B) = P(A) + P(B) - P(A \cap B)
$$

If $A$ and $B$ are mutually exclusive:

$$
P(A \cup B) = P(A) + P(B)
$$

---

## Conditional Probability

The **conditional probability** of event $A$ given that event $B$ has occurred is defined as:

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0
$$

Conditional probability allows us to update probabilities based on additional information.

---

## Independence of Events

Two events $A$ and $B$ are said to be **independent** if the occurrence of one does not affect the probability of the other.

$$
P(A \cap B) = P(A)P(B)
$$

Independence is a strong assumption and must be justified carefully in practice.

---

## Bayes’ Theorem

Bayes’ theorem provides a way to reverse conditional probabilities.

$$
P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}
$$

Bayes’ theorem is fundamental in medical testing, machine learning, and decision theory.

---

## Random Variables

A **random variable** is a function that assigns a numerical value to each outcome in the sample space.

- **Discrete random variables** take countable values
- **Continuous random variables** take values over an interval

Examples include:

- Number of heads in coin tosses
- Height or weight of individuals

---

## Probability Distributions

A **probability distribution** describes how probabilities are assigned to values of a random variable.

Examples:

- Discrete: Bernoulli, Binomial, Poisson
- Continuous: Uniform, Normal, Exponential

Detailed study of distributions will appear in later chapters.

---

## Expectation and Variance

For a random variable $X$:

- **Expected value** measures the long-run average
- **Variance** measures variability around the mean

These quantities play a central role in statistical modeling and inference.

---

## Law of Large Numbers (Conceptual)

The **Law of Large Numbers** states that as the number of trials increases, the sample mean converges to the expected value.

This principle explains why probabilities manifest as stable long-run frequencies.

---

## Probability and Simulation in R

R can be used to simulate random experiments and illustrate probability concepts.

``` r
set.seed(123)
coin_tosses <- sample(c("H", "T"), size = 1000, replace = TRUE)
mean(coin_tosses == "H")
```

Simulation provides intuition when analytical solutions are complex.

------------------------------------------------------------------------

## Common Misconceptions in Probability

-   Confusing independence with mutual exclusivity
-   Misinterpreting conditional probabilities
-   Assuming short-term frequencies must match probabilities
-   Ignoring base rates in real-world problems

Awareness of these pitfalls is essential for sound reasoning.

------------------------------------------------------------------------

## Summary

This chapter introduced the fundamental ideas of **probability theory**. You learned about:

-   Random experiments, sample spaces, and events
-   Definitions and axioms of probability
-   Conditional probability and independence
-   Bayes’ theorem
-   Random variables and probability distributions
-   The role of simulation in understanding probability

These concepts form the mathematical backbone for **statistical inference**, which will be developed in the next chapter.
